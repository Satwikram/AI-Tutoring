{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgtA8fcbZuTIKvPlq3yVXz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Satwikram/AI-Tutoring/blob/main/NLP/FineTuning%20BERT%20for%20TEXT%20classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "bCxry02tnl9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install plotly\n",
        "!pip install livelossplot"
      ],
      "metadata": {
        "id": "buk_HGFenh7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()\n",
        "\n",
        "\n",
        "! mkdir ~/.kaggle\n",
        "\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "PT4rpJV3n1-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading the Dataset"
      ],
      "metadata": {
        "id": "Kz6_CIUCnsmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d rmisra/news-headlines-dataset-for-sarcasm-detection"
      ],
      "metadata": {
        "id": "gAEeXdbknq8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/news-headlines-dataset-for-sarcasm-detection.zip"
      ],
      "metadata": {
        "id": "9D3BUjL9nzbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "z8TI6IHrm0WO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4khRVFprdV3M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "import plotly.express as px\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau, LearningRateScheduler\n",
        "\n",
        "import spacy\n",
        "from unicodedata import normalize\n",
        "\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from livelossplot import PlotLossesKeras"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Dataset"
      ],
      "metadata": {
        "id": "uS3J5aYSonEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json(\"/content/Sarcasm_Headlines_Dataset_v2.json\", lines = True)"
      ],
      "metadata": {
        "id": "o7Xk55flm24h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "67eb0phJp1ZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "dKEwP5Mjp3Zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bar = df['is_sarcastic'].value_counts()\n",
        "print(bar)\n",
        "bar.plot(kind=\"bar\")"
      ],
      "metadata": {
        "id": "g7bGQtuQXuxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning the Texts"
      ],
      "metadata": {
        "id": "DdMewmAquzut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def clean_data(df, column):\n",
        "\n",
        "  def lem_stp():\n",
        "\n",
        "    for doc in nlp.pipe(df[column], disable=[\"parser\", \"ner\"], batch_size=512):\n",
        "        yield \" \".join(\n",
        "            [d.lemma_ for d in doc if not d.is_stop]\n",
        "        )\n",
        "  \n",
        "  def clean(text):\n",
        "\n",
        "    text = str(text).strip()\n",
        "\n",
        "    if text:\n",
        "      \n",
        "      #Normalize Text\n",
        "      text = normalize(\"NFKD\", text)\n",
        "\n",
        "      #Remove links \n",
        "      text = re.sub(r'https?:\\/\\/.*?[\\s+]', '', text.replace(\"|\",\" \") + \" \")\n",
        "\n",
        "      #Strip Punctation\n",
        "      text = re.sub(r'[^\\w\\s]','', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "  df[column] = df[column].apply(clean)\n",
        "  df[column] = list(lem_stp())\n",
        "  df[column] = df[column].apply(lambda x: re.sub(\"\\s+\", \" \", x.strip()))\n",
        "  df[column] = df[column].apply(lambda x: x if len(x.split()) >= 5 else None)\n",
        "\n",
        "  return df[column]"
      ],
      "metadata": {
        "id": "ua2nLHXOuMRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Cleaned\"] = clean_data(df, \"headline\")\n",
        "df.dropna(inplace=True)\n",
        "df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "XHJvqKk2wq3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[\"Cleaned\"]\n",
        "y = df[\"is_sarcastic\"]"
      ],
      "metadata": {
        "id": "64sb52VUwzdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating Sequence Length"
      ],
      "metadata": {
        "id": "z8onZLppxVv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "length = [len(x.split()) for x in X]"
      ],
      "metadata": {
        "id": "Wos5g4LuyXTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "px.box(length)"
      ],
      "metadata": {
        "id": "9l7sTtKTyfk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ],
      "metadata": {
        "id": "W7Mil0CZxMJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"bert-base-uncased\"\n",
        "# checkpoint = \"gpt2\"\n",
        "sequence_length = 64\n",
        "\n",
        "def tokenize(samples):\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "  if checkpoint == \"gpt2\" and tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "  tokens = tokenizer(\n",
        "      samples,\n",
        "      max_length=sequence_length,\n",
        "      truncation=True,\n",
        "      padding=\"max_length\",\n",
        "      add_special_tokens=True,\n",
        "      return_tensors=\"np\"\n",
        "  )\n",
        "\n",
        "  return {\"input_ids\": tokens[\"input_ids\"].tolist(), \"attention_mask\": tokens[\"attention_mask\"].tolist()}"
      ],
      "metadata": {
        "id": "HlZipeGQw_FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_tokenized = pd.DataFrame(tokenize(X.tolist()), columns=[\"input_ids\", \"attention_mask\"])"
      ],
      "metadata": {
        "id": "EzDDbI34zdz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_tokenized"
      ],
      "metadata": {
        "id": "CSPuLvrGzzJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_tokenized.loc[0]"
      ],
      "metadata": {
        "id": "jHeYXXRzz-Tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "g9H9KY3S0Z_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting Data into Train/Test"
      ],
      "metadata": {
        "id": "yFFRiT6A0L0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_tokenized, y, test_size=0.2, random_state=42, shuffle=True, stratify=y)"
      ],
      "metadata": {
        "id": "V1oz_j0Y0AoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "M-zM8SKQ0dhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "metadata": {
        "id": "ASsQLmUy0euS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unzipping the IDs and Masks"
      ],
      "metadata": {
        "id": "dFXIzZDJ1OWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[\"input_ids\"][0]"
      ],
      "metadata": {
        "id": "6Pz4OPHr1jn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unzip_x = lambda x: [np.vstack(x[\"input_ids\"]), np.vstack(x[\"attention_mask\"])]\n",
        "\n",
        "X_train, X_test = unzip_x(X_train), unzip_x(X_test)"
      ],
      "metadata": {
        "id": "4caG9s5J0lS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "82rqfO0_2BNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the model"
      ],
      "metadata": {
        "id": "61_Rk6Ev2oDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(df, targets, checkpoint, sequence_length):\n",
        "\n",
        "  base_model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "  input_ids = Input(shape=(sequence_length,), name=\"input_ids\", dtype=\"int32\")\n",
        "  attenion_mask = Input(shape=(sequence_length,), name=\"attention_mask\", dtype=\"int32\")\n",
        "\n",
        "  if checkpoint == \"gpt2\": x = base_model.transformer(input_ids, attention_mask=attenion_mask)[0]\n",
        "  else: x = base_model.bert(input_ids, attention_mask=attenion_mask)[1]\n",
        "\n",
        "  x = Flatten()(x)\n",
        "\n",
        "  units = df[targets].nunique()\n",
        "\n",
        "  if units > 2:\n",
        "      activation = \"softmax\"\n",
        "      loss = \"sparse_categorical_crossentropy\"\n",
        "  else:\n",
        "      activation = \"sigmoid\"\n",
        "      loss = \"binary_crossentropy\"\n",
        "      units = units - 1\n",
        "\n",
        "  outputs = Dense(units, activation = activation, name = f\"{targets}_outputs\")(x)\n",
        "\n",
        "  model = Model(inputs=[input_ids, attenion_mask], outputs=outputs)\n",
        "\n",
        "  optimizer =  tf.keras.optimizers.Adam()\n",
        "\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
        "\n",
        "  # Model Architecture Export\n",
        "  tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True, \n",
        "                          show_dtype=True, show_layer_names=True, rankdir='TB',\n",
        "                          expand_nested=True, dpi=300, layer_range=None, \n",
        "                          show_layer_activations=True)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "rvL8UxRa2CRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(df, \"is_sarcastic\", checkpoint, sequence_length)"
      ],
      "metadata": {
        "id": "AQSMrlas33o7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "N_4F3WoI3_AM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Callbacks"
      ],
      "metadata": {
        "id": "urd04R0F4z3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def callbacks() -> list:\n",
        "\n",
        "    run_name = \"run 1\"\n",
        "    save_path = Path(\"models\")\n",
        "    os.makedirs(save_path/\"logs\", exist_ok=True)\n",
        "    \n",
        "    checkpoint = ModelCheckpoint(save_path, monitor=\"val_loss\", save_best_only=True, \n",
        "                                                    verbose=1)\n",
        "\n",
        "    earlystopping = EarlyStopping(monitor=\"val_loss\", verbose=1, restore_best_weights = True,\n",
        "                                                    patience=5)\n",
        "\n",
        "    logger = TensorBoard(save_path/\"logs\"/run_name, histogram_freq=2, write_graph=True, write_images=True)\n",
        "\n",
        "    lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=2, verbose=1,\n",
        "                                         min_delta=0.0001, cooldown=0, min_lr=0)\n",
        "    \n",
        "    return [checkpoint, earlystopping, lr, logger, PlotLossesKeras()]"
      ],
      "metadata": {
        "id": "wa-MxchR4wZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "7TzczzDS62wT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=8, callbacks=callbacks())"
      ],
      "metadata": {
        "id": "UXuh5zOm6ziF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5EKnFNX267WP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}